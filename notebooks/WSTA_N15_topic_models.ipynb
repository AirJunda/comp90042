{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Notebook for COMP90042, Web search and Text Analysis*\n",
    "\n",
    "*Copyright The University of Melbourne, 2018*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will use Gensim to train topic models on the Brown corpus. For this notebook, we will consider paragraphs as documents, for the sake of efficiency. In a real world scenario, you will probably deal with full documents instead. Nevertheless, the steps provided here also apply to documents as well.\n",
    "\n",
    "Let's start by reading the Brown corpus as a list of paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "docs = list(brown.paras())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train a topic model on this data using Gensim. There are a range of models available but for this notebook we will stick to standard Latent Dirichlet Allocation (LDA).\n",
    "\n",
    "Before we do that though, we need to preprocess the data a bit in order for it to be read by Gensim: 1) we will flatten each document into a single list; 2) build a dictionary mapping words to ids and 3) generate a bag-of-words representation for each document using the word ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim as gs\n",
    "\n",
    "flat_docs = [[w for s in d for w in s] for d in docs]\n",
    "brown_dict = gs.corpora.dictionary.Dictionary(flat_docs)\n",
    "bow_docs = [brown_dict.doc2bow(d) for d in flat_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train a topic model. While we could use EM for this, the standard way to train is to estimate full posterior distribution for the parameters. This is a complex procedure that is out of the scope of the module but luckily Gensim has this implemented so we can just treat it as a black box procedure.\n",
    "\n",
    "Notice we give a few parameters to the model:\n",
    "\n",
    "- The number of topics.\n",
    "- The dictionary.\n",
    "- How many passes in the training data. This relates to training algorithm but to simplify: more passes usually better but take longer.\n",
    "- The random state, since training is not deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ldamodel = gs.models.ldamodel.LdaModel(bow_docs, num_topics=10, id2word=brown_dict, \n",
    "                                       passes=20, random_state=np.random.RandomState(10))\n",
    "print(ldamodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now inspect the learned topics. To do this, we will print word lists for each topic and manually inspect if we can infer any meaning from these lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ldamodel.print_topics(num_words=20)\n",
    "print(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that for every topic we have a list of numbers/words. The numbers represent the probability of word appearing given the topic (check this). However, this output is hard to interpret so let's format to a more friendly format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pprint_topics(ldamodel, num_words=20):\n",
    "    topics = ldamodel.print_topics(num_words=num_words)\n",
    "    word_lists = [(t[0], t[1]) for t in topics]\n",
    "    word_lists = [(t[0], [w.split('*')[1] for w in t[1].split(' + ')]) for t in word_lists]\n",
    "    topic_ids = [t[0] for t in word_lists]\n",
    "    word_lists = [' '.join([w[1:-1] for w in t[1]]) for t in word_lists]\n",
    "    for t_id, w_list in zip(topic_ids, word_lists):\n",
    "        print('%d:\\t%s' % (t_id, w_list))\n",
    "\n",
    "pprint_topics(ldamodel, num_words=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard to understand what's happening, right? This is because we did not do any preprocessing on the corpus. Similar to what we do in text classification, we should preprocess the corpus before training a topic model. Here though, the issue is much more evident as we end up with very uninterpretable topics.\n",
    "\n",
    "So let's do some preprocessing steps. These might take a few seconds to run.\n",
    "\n",
    "- lowercase words\n",
    "- ignore punctuation\n",
    "- remove stopwords\n",
    "- lemmatise words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "stopwords = list(nltk.corpus.stopwords.words())\n",
    "\n",
    "def preprocess_docs(corpus, stopwords):\n",
    "    new_corpus = []\n",
    "    for doc in corpus:\n",
    "        new_doc = []\n",
    "        for word in doc:\n",
    "            if not word.isalpha():\n",
    "                continue\n",
    "            new_word = word.lower()\n",
    "            if new_word in stopwords:\n",
    "                continue\n",
    "            new_word = lemmatize(new_word)\n",
    "            new_doc.append(new_word)\n",
    "        new_corpus.append(new_doc)\n",
    "    return new_corpus\n",
    "\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "  \n",
    "\n",
    "filtered_docs = preprocess_docs(flat_docs, stopwords)\n",
    "brown_dict = gs.corpora.dictionary.Dictionary(filtered_docs)\n",
    "bow_docs = [brown_dict.doc2bow(d) for d in filtered_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train a new topic model on the filtered data and check what we come up with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = gs.models.ldamodel.LdaModel(bow_docs, num_topics=10, id2word=brown_dict, \n",
    "                                       passes=20, random_state=np.random.RandomState(10))\n",
    "pprint_topics(ldamodel, num_words=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better now, right? Not every topic is 100% interpretable but some insights can be made. Notice that some words appear in more than one topic: this is expected in LDA (revisit the reading material and slides if you do not understand why). Can you find appropriate labels for some of the topics?\n",
    "\n",
    "From here there are plenty of things you can experiment with. Check the Gensim website (https://radimrehurek.com/gensim/) for documentation and tutorials. Here a few suggestions:\n",
    "\n",
    "- For visualisation, you can increase the number of words and/or try to come up with some filtering such as printing nouns and verbs only (how would you do that?).\n",
    "- Inspect some documents in the corpus and check their topic distribution. You should check out methods in the Gensim API for that.\n",
    "- Change the number of topics in LDA.\n",
    "- Train on a different corpus, such as the Twitter samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
