{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vector learning with Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will help you get started with the skip-gram and CBOW model of word vector learning. Here we'll be using the 'gensim' toolkit for python, which reimplements the code for training and evaluation of the models. Feel free to play with the orginal C code at https://code.google.com/archive/p/word2vec/ also."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to load the library into python. Note that you will need 'gensim' and its dependent libraries as well as 'nltk' installed in your environment. If you see an error importing these below, you'll need to 'pip install' a few packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim \n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this we will use part of the Brown corpus. Note that for the word vectors to be meaningful you need much more text than this. However the code will run slowly, so we'll just consider a subset for the moment. In general you need to use much bigger corpora, e.g., all of Wikipedia, and train the model for hours or days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "corpus = nltk.corpus.brown.sents()[:5000] # make the corpus smaller so this code runs quickly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next line trains the model on the corpus, modelling all words that occur in the corpus. You can prune the vocabulary using the 'min_count' option, such that only words seen several times are included. This helps to make things faster and avoid modelling what might be typographical errors or other noise in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(corpus, min_count=1, workers=2, iter=5) # workers = number of threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the various options for training the model, please read the help (see below, this will pop up a help window). For example the 'sg' option allows you to swap between the CBOW and skip-gram models. You can compare the results for running with both models by rerunning the training command above with this option. Consider 'size' also, which sets the number of hidden dimensions. There are a plethora of other parameters which can also have a big effect on the model quality and runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gensim.models.Word2Vec?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going any further, you probably want to save your model to avoid the need to re-run the slow training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('brown_skipgram.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's interrogate the model to see how similar various words are. Please try out some words yourself, to see whether its uncovered sensible or otherwise interesting relations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = \"woman women man girl boy green blue did\".split()\n",
    "for w1 in words:\n",
    "    for w2 in words:\n",
    "        print w1, w2, model.similarity(w1, w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also find the 'k' most similar words to a given word. There's a lot of gibberish when trained on a small corpus. You should try increasing the size of the training data to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=['woman'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have more of a play with the 'most_similar' method, using the 'negative' option to perform vector difference in order to evaluate king-man+woman = ??? and similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "your code goes here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more quantitative evaluation, download this file to the current working directory http://word2vec.googlecode.com/svn/trunk/questions-words.txt and see if your model gets many of these values correct. You will want to train on the full Brown corpus to give the model enough data to learn some of these relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.accuracy('questions-words.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally you can download a model trained on masses of data. Be warned, the parameter file is 1.5GiB compressed! The runtime memory usage can be high too ~10GiB. See https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "big_model = model.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the big model perform a lot better? Come up with some tests for different relation types, e.g., city-state, place-sport team, hyper-hyponym, tense, number, etc and see if the model can capture these well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
