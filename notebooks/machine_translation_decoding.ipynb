{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation: The phrase-based approach and decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll be looking at the phrase-based SMT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import itertools\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate import *\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our problem is to model *Centauri-Arctuan* translation, given no knowledge of these alien languages (ok, they're made up.) The first component of the dataset is short  bitext:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = \"\"\"ok-voon ororok sprok .             |||  at-voon bichat dat .\n",
    "ok-drubel ok-voon anok plok sprok .          |||  at-drubel at-voon pippat rrat dat .\n",
    "erok sprok izok hihok ghirok .               |||  totat dat arrat vat hilat .\n",
    "ok-voon anok drok brok jok .                 |||  at-voon krat pippat sat lat .\n",
    "wiwok farok izok stok .                      |||  totat jjat quat cat .\n",
    "lalok sprok izok jok stok .                  |||  wat dat krat quat cat .\n",
    "lalok farok ororok lalok sprok izok enemok . |||  wat jjat bichat wat dat vat eneat .\n",
    "lalok brok anok plok nok .                   |||  iat lat pippat rrat nnat .\n",
    "wiwok nok izok kantok ok-yurp .              |||  totat nnat quat oloat at-yurp .\n",
    "lalok mok nok yorok ghirok clok .            |||  wat nnat gat mat bat hilat .\n",
    "lalok nok crrrok hihok yorok zanzanok .      |||  wat nnat arrat mat zanzanat .\n",
    "lalok rarok nok izok hihok mok .             |||  wat nnat forat arrat vat gat .\"\"\"\n",
    "\n",
    "bitext = []\n",
    "for line in d.split('\\n'):\n",
    "    en, fr = line.split('|||')\n",
    "    bitext.append(AlignedSent(word_tokenize(en), word_tokenize(fr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a monolingual *Centauri* (called E herein) text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = \"\"\"ok-drubel anok ghirok farok . wiwok rarok nok zerok ghirok enemok .\n",
    "ok-drubel ziplok stok vok erok enemok kantok ok-yurp zinok jok yorok clok .\n",
    "lalok clok izok vok ok-drubel . \n",
    "ok-voon ororok sprok . \n",
    "ok-drubel ok-voon anok plok sprok . \n",
    "erok sprok izok hihok ghirok . \n",
    "ok-voon anok drok brok jok . \n",
    "wiwok farok izok stok . \n",
    "lalok sprok izok jok stok . \n",
    "lalok brok anok plok nok . \n",
    "lalok farok ororok lalok sprok izok enemok . \n",
    "wiwok nok izok kantok ok-yurp . \n",
    "lalok mok nok yorok ghirok clok . \n",
    "lalok nok crrrok hihok yorok zanzanok . \n",
    "lalok rarok nok izok hihok mok . \"\"\"\n",
    "monolingual = word_tokenize(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And some test sentences in *Arctuan* (called F herein):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['iat', 'lat', 'pippat', 'eneat', 'hilat', 'oloat', 'at-yurp', '.'],\n",
       " ['totat', 'nnat', 'forat', 'arrat', 'mat', 'bat', '.'],\n",
       " ['wat', 'dat', 'quat', 'cat', 'uskrat', 'at-drubel', '.']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = \"\"\"iat lat pippat eneat hilat oloat at-yurp .\n",
    "totat nnat forat arrat mat bat .\n",
    "wat dat quat cat uskrat at-drubel .\"\"\"\n",
    "test = [ word_tokenize(sent) for sent in d.split(\"\\n\")]\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll obtain some word alignments using IBM model 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function nltk.translate.ibm_model.<lambda>>,\n",
       "            {None: 2.4549317962112147e-09,\n",
       "             '.': 4.700398025663745e-10,\n",
       "             'arrat': 9.730875743986166e-10,\n",
       "             'at-drubel': 6.487252144146402e-10,\n",
       "             'at-voon': 2.2928996232397085e-10,\n",
       "             'bichat': 2.461320250665924e-10,\n",
       "             'cat': 4.865439399167633e-10,\n",
       "             'dat': 0.9999999948024947,\n",
       "             'eneat': 4.531193844994274e-10,\n",
       "             'hilat': 1.4596313622543465e-09,\n",
       "             'jjat': 2.2655969222525066e-10,\n",
       "             'krat': 4.865439473661642e-10,\n",
       "             'pippat': 2.1624174032085257e-10,\n",
       "             'quat': 3.2436262664067507e-10,\n",
       "             'rrat': 3.243626072073094e-10,\n",
       "             'totat': 9.730875743553228e-10,\n",
       "             'vat': 1.1241273694343646e-09,\n",
       "             'wat': 3.126217439229138e-10})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = IBMModel3(bitext, 5)\n",
    "# show the translations of one of the words\n",
    "m.translation_table['sprok']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what it learned (also try some other sentences, other than just sentence pair 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.36.0 (20140111.2315)\n",
       " -->\n",
       "<!-- Title: align Pages: 1 -->\n",
       "<svg width=\"458pt\" height=\"116pt\"\n",
       " viewBox=\"0.00 0.00 458.00 116.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 112)\">\n",
       "<title>align</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-112 454,-112 454,4 -4,4\"/>\n",
       "<!-- erok_source -->\n",
       "<g id=\"node1\" class=\"node\"><title>erok_source</title>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">erok</text>\n",
       "</g>\n",
       "<!-- sprok_source -->\n",
       "<g id=\"node2\" class=\"node\"><title>sprok_source</title>\n",
       "<text text-anchor=\"middle\" x=\"99\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">sprok</text>\n",
       "</g>\n",
       "<!-- erok_source&#45;&#45;sprok_source -->\n",
       "<!-- totat_target -->\n",
       "<g id=\"node7\" class=\"node\"><title>totat_target</title>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">totat</text>\n",
       "</g>\n",
       "<!-- erok_source&#45;&#45;totat_target -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>erok_source&#45;&#45;totat_target</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M27,-71.6966C27,-60.8463 27,-46.9167 27,-36.1043\"/>\n",
       "</g>\n",
       "<!-- izok_source -->\n",
       "<g id=\"node3\" class=\"node\"><title>izok_source</title>\n",
       "<text text-anchor=\"middle\" x=\"189\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">izok</text>\n",
       "</g>\n",
       "<!-- sprok_source&#45;&#45;izok_source -->\n",
       "<!-- dat_target -->\n",
       "<g id=\"node8\" class=\"node\"><title>dat_target</title>\n",
       "<text text-anchor=\"middle\" x=\"99\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">dat</text>\n",
       "</g>\n",
       "<!-- sprok_source&#45;&#45;dat_target -->\n",
       "<g id=\"edge6\" class=\"edge\"><title>sprok_source&#45;&#45;dat_target</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M99,-71.6966C99,-60.8463 99,-46.9167 99,-36.1043\"/>\n",
       "</g>\n",
       "<!-- hihok_source -->\n",
       "<g id=\"node4\" class=\"node\"><title>hihok_source</title>\n",
       "<text text-anchor=\"middle\" x=\"261\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">hihok</text>\n",
       "</g>\n",
       "<!-- izok_source&#45;&#45;hihok_source -->\n",
       "<!-- vat_target -->\n",
       "<g id=\"node10\" class=\"node\"><title>vat_target</title>\n",
       "<text text-anchor=\"middle\" x=\"279\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">vat</text>\n",
       "</g>\n",
       "<!-- izok_source&#45;&#45;vat_target -->\n",
       "<g id=\"edge5\" class=\"edge\"><title>izok_source&#45;&#45;vat_target</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M211.247,-71.6966C225.198,-60.8463 243.107,-46.9167 257.009,-36.1043\"/>\n",
       "</g>\n",
       "<!-- ghirok_source -->\n",
       "<g id=\"node5\" class=\"node\"><title>ghirok_source</title>\n",
       "<text text-anchor=\"middle\" x=\"351\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">ghirok</text>\n",
       "</g>\n",
       "<!-- hihok_source&#45;&#45;ghirok_source -->\n",
       "<!-- arrat_target -->\n",
       "<g id=\"node9\" class=\"node\"><title>arrat_target</title>\n",
       "<text text-anchor=\"middle\" x=\"207\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">arrat</text>\n",
       "</g>\n",
       "<!-- hihok_source&#45;&#45;arrat_target -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>hihok_source&#45;&#45;arrat_target</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M247.652,-71.6966C239.281,-60.8463 228.536,-46.9167 220.195,-36.1043\"/>\n",
       "</g>\n",
       "<!-- ._source -->\n",
       "<g id=\"node6\" class=\"node\"><title>._source</title>\n",
       "<text text-anchor=\"middle\" x=\"423\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">.</text>\n",
       "</g>\n",
       "<!-- ghirok_source&#45;&#45;._source -->\n",
       "<!-- hilat_target -->\n",
       "<g id=\"node11\" class=\"node\"><title>hilat_target</title>\n",
       "<text text-anchor=\"middle\" x=\"351\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">hilat</text>\n",
       "</g>\n",
       "<!-- ghirok_source&#45;&#45;hilat_target -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>ghirok_source&#45;&#45;hilat_target</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M351,-71.6966C351,-60.8463 351,-46.9167 351,-36.1043\"/>\n",
       "</g>\n",
       "<!-- ._target -->\n",
       "<g id=\"node12\" class=\"node\"><title>._target</title>\n",
       "<text text-anchor=\"middle\" x=\"423\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">.</text>\n",
       "</g>\n",
       "<!-- ._source&#45;&#45;._target -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>._source&#45;&#45;._target</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M423,-71.6966C423,-60.8463 423,-46.9167 423,-36.1043\"/>\n",
       "</g>\n",
       "<!-- totat_target&#45;&#45;dat_target -->\n",
       "<!-- dat_target&#45;&#45;arrat_target -->\n",
       "<!-- arrat_target&#45;&#45;vat_target -->\n",
       "<!-- vat_target&#45;&#45;hilat_target -->\n",
       "<!-- hilat_target&#45;&#45;._target -->\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "AlignedSent(['erok', 'sprok', 'izok', 'hihok', 'ghirok', '.'], ['totat', 'dat', 'arrat', 'vat', 'hilat', '.'], Alignment([(0, 0), (1, 1), (2, 3), (3, 2), (4, 4), (5, 5)]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = bitext[2]\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see the warning message *Cannot find the dot binary from Graphviz package* you might want to install [Graphviz](http://www.graphviz.org/Download.php). Alternatively, you can view the aligned sentences in text using print:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlignedSent(['erok', 'sprok', 'izok', 'hihok', 'ghirok', '.'], ['totat', 'dat', 'arrat', 'vat', 'hilat', '.'], Alignment([(0, 0), (1, 1), (2, 3), (3, 2), (4, 4), (5, 5)]))\n"
     ]
    }
   ],
   "source": [
    "print `s`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also align in the reverse direction, and combine the alignments. But for the purpose of this notebook, we won't bother with this improvement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in building a phrase-based system is to extract the phrases that are consistent with the alignment, in order to construct the phrase-table. Let's look at the phrases we can obtain from this sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{((0, 1), (0, 1), 'erok', 'totat'),\n",
       " ((0, 2), (0, 2), 'erok sprok', 'totat dat'),\n",
       " ((1, 2), (1, 2), 'sprok', 'dat'),\n",
       " ((1, 4), (1, 4), 'sprok izok hihok', 'dat arrat vat'),\n",
       " ((2, 3), (3, 4), 'izok', 'vat'),\n",
       " ((2, 4), (2, 4), 'izok hihok', 'arrat vat'),\n",
       " ((2, 5), (2, 5), 'izok hihok ghirok', 'arrat vat hilat'),\n",
       " ((3, 4), (2, 3), 'hihok', 'arrat'),\n",
       " ((4, 5), (4, 5), 'ghirok', 'hilat'),\n",
       " ((4, 6), (4, 6), 'ghirok .', 'hilat .'),\n",
       " ((5, 6), (5, 6), '.', '.')}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.translate.phrase_based import phrase_extraction\n",
    "phrase_extraction(' '.join(s.words), ' '.join(s.mots), s.alignment, max_phrase_length=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this only a small subset of the possible spans over the two sentences. Some are some spans that aren't valid, such as *sprok izok* (1,3). Can you explain why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the phrase table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the full phrase-table we will extract all phrase-pairs (as above) and store their counts. Finally we normalise to give a conditional probability of $p(f|e)$ (roughly speaking.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phrase_pair_counts = defaultdict(Counter)\n",
    "for s in bitext:\n",
    "    phrases = phrase_extraction(' '.join(s.words), ' '.join(s.mots), s.alignment, max_phrase_length=3)\n",
    "    for en_span, fr_span, en_phrase, fr_phrase in phrases:\n",
    "        phrase_pair_counts[tuple(en_phrase.split())][tuple(fr_phrase.split())] += 1\n",
    "#phrase_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then normalise their counts to form conditional probabilities, such that they sum to one. We also take the logarithm, because the decoder sums log probabilities rather than multiplies probabilities (to avoid *floating point underflow*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phrase_table = PhraseTable()\n",
    "for en_phrase, fr_counts in phrase_pair_counts.items():\n",
    "    total = sum(fr_counts.values())\n",
    "    for fr_phrase, count in fr_counts.items():\n",
    "        phrase_table.add(fr_phrase, en_phrase, log(count / float(total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should inspect the learned values, and you'll see that most probabilities are 1 (i.e., 0 in log space.) Can you explain why this is the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to build a language model. We'll use a simple unigram model, with no smoothing. Of course in practice you'd use a higher order model, usually between 3-5 grams and with fancy smoothing such as modified Kneser-Ney."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count the words\n",
    "unigram_counts = Counter()\n",
    "for token in monolingual:\n",
    "    unigram_counts[token] += 1\n",
    "    \n",
    "# normalise and take logarithm\n",
    "language_prob = defaultdict(lambda: -999.0)\n",
    "total_words = sum(unigram_counts.values())\n",
    "for word_type, count in unigram_counts.items():\n",
    "    language_prob[word_type] = log(count / float(total_words))\n",
    "#language_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of the *StackDecoder* in NLTK, we need to wrap the LM using an object with methods to score the LM probabilities wholly within a phrase for future cost estimation (*probability*) and in context for use when adding nodes to the decoding graph (*probability_change*). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LM:\n",
    "    def __init__(self, probs):\n",
    "        self.probs = probs        \n",
    "    def probability_change(self, context, phrase): \n",
    "        # Used when expanding a hypothesis with a new phrase\n",
    "        # (higher order LMs would need to look at the word sequence, including context)\n",
    "        return sum([self.probs[word] for word in phrase])\n",
    "    def probability(self, phrase): \n",
    "        # Used for future cost estimation only, to get a cheap (approximate) LM score for each phrase \n",
    "        return sum([self.probs[word] for word in phrase])\n",
    "language_model = LM(language_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the decoder with our phrase-table and LM, and ask it to translate a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lalok', 'brok', 'anok', 'enemok', 'ghirok', 'kantok', 'ok-yurp', '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_decoder = StackDecoder(phrase_table, language_model)\n",
    "stack_decoder.translate(test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out some of the other test sentences. Does it always succeed in translating all of them? (What might might cause it to fail?) And are the translations any good - with a unigram LM, there's no incentive to reorder the words, which could be detrimental... You might want to try moving to a bigram LM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve on this, you will need to delve deeper into the code of the phrase-based stack decoder. Please refer to [the nltk.translate API documentation and source code](http://www.nltk.org/_modules/nltk/translate/stack_decoder.html). Note that this is only a toy implementation, which is great for learning purposes, although for realistic settings tools like [the moses open source decoder](http://www.statmt.org/moses/) are more suitable. You can see how the phrase-based model can be used in moses in the [moses tutorial](http://www.statmt.org/moses/?n=Moses.Tutorial)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're curious about the data, please see the excellent [article by Kevin Knight about learning translation models using EM](http://www.ai.mit.edu/courses/6.891-nlp/READINGS/aimag97.pdf) where it forms the running example. (Yes, it's not really alien language, but it is based on actual human languages.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
