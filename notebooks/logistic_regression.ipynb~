{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression (from scratch!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks you through the process of training & classifying with a logistic regression model. This is to provide your the chance to better understand the working of the model. For a more practical end-user variant of the same method, please see the text classification notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import fmin_bfgs\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some simple 2d data to demonstrate logistic regression. Note that usually we'll work with more than 2 dimensions, however for the sake of plotting the results we'll stick to 2d data for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.random.normal(size=(20, 3))\n",
    "X[:,0] = 1\n",
    "X[:10,1:] += 1\n",
    "X[10:,1:] -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there's a the first column (0) is added as the constant 1, which is a clever shortcut for adding a bias (constant), with coefficient $w_0$. This avoids the need to consider an explicit bias term, which is important for toy examples (but often folded into your features on real data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = np.hstack([[0] * 10, [1] * 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the random data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(X[y==0,1], X[y==0,2], 'o')\n",
    "plt.plot(X[y==1,1], X[y==1,2], 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hope to find a nice classifier that separates the blue and green points, i.e., with a diagonal line sloping down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training logistic regression requires computing the *cross-entropy* objective, which is defined as $$\\log \\prod_i P(y_i | \\mathbf{x}_i) = \\log \\sum_i \\log P(y_i | \\mathbf{x}_i)$$\n",
    "with a penalty term on the weight magnitude. We maximise this with respect to the weights **w** using the **BFGS** gradient optimiser. This requires the gradient of the objective, as defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training objective, the \"cross entropy\" or log probability of the training data \n",
    "def objective(w, X, y, ss):\n",
    "   # takes parameter vector, w, matrix of training inputs X, training output vector (of 0s and 1s) y, and prior term \n",
    "   # predictive probability is the logistic sigmoid of x.w \n",
    "   prob_y = 1./(1.+np.exp(-np.dot(X,w)))\n",
    "   #      regularisation term     +     log probability of class 1    +   log probablity of class 0 \n",
    "   obj = -(1./(2*ss))*np.dot(w,w) + np.dot(y, np.log(prob_y)) + np.dot(1. - y, np.log(1. - prob_y))\n",
    "   return -obj\n",
    "\n",
    "# for binary data y is either 0 or 1, so the dot products with y and 1-y sum up the \n",
    "# predictive log probabilities for just the 0 labelled instances, or 1 labelled instances\n",
    "# (this is a clever trick that allows us to avoid writing a loop; and tends to run much faster)\n",
    "\n",
    "# the gradient of the above with respect to weights w \n",
    "def dobj_dw(w, X, y, ss):\n",
    "   prob_y = 1./(1.+np.exp(-np.dot(X, w)))\n",
    "   grad = -(1./ss)*w + np.dot(y - prob_y, X)\n",
    "   return -grad\n",
    "# please see the lecture & reading for the maths; but basically it's the partial derivative of the\n",
    "# objective function above with respect to w\n",
    "\n",
    "# display function called each iteration of BFGS\n",
    "plt.plot(X[y==0,1], X[y==0,2], 'o')\n",
    "plt.plot(X[y==1,1], X[y==1,2], 's')\n",
    "\n",
    "def display(w):\n",
    "    # add a line to the plot showing the decision boundary, where \n",
    "    # the classification is tied, Pr(y=0) = Pr(y=1)\n",
    "    # (either side of this line correspond to class 0 and 1)\n",
    "    xdisp = np.arange(-2, 2, 0.01)\n",
    "    ydisp = -(w[0] + w[1] * xdisp)/w[2]\n",
    "    plt.plot(xdisp, ydisp)\n",
    "    print 'w is', w, 'objective is', objective(w, X, y, ss)\n",
    "\n",
    "# minimise using BFGS gradient based solver\n",
    "ss = 1\n",
    "w = fmin_bfgs(f=objective, fprime=dobj_dw, x0=np.array([0,0,0]), \n",
    "              args=(X, y, ss), disp=True, callback=display)\n",
    "\n",
    "plt.legend(['y=0', 'y=1'] + ['it %d' % i for i in range(1,20)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course in practise you'd use a more optimised method such as logistic regression implementations in Scikit-learn, Theano, R etc. The version above is likely to have problems with floating point underflow and overflow.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running this a few times on fresh random data (rerun the commands at the top), until you get a linearly separable problem. In this case what happens as you change ss, e.g., to 0 (or near to zero, e.g., 1e-10) to remove the effect of the regularisation term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The objective space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LBFGS optimiser called above did model fitting using gradient descent. It's instructive to take a look at the objective function directly, which we can do here as we have so few dimensions. We'll ignore the bias term, and plot the training objective as a function of the two other weights.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.mlab as mlab\n",
    "\n",
    "delta = 0.025\n",
    "w1, w2 = np.meshgrid(np.arange(w[1]-2.0, w[1]+2.0, delta), np.arange(w[2]-2.0, w[2]+2.0, delta))\n",
    "ws = np.vstack([w[0] * np.ones_like(w1.flatten()), w1.flatten(), w2.flatten()])\n",
    "obj = np.zeros(ws.shape[1])\n",
    "for i in range(ws.shape[1]):\n",
    "    obj[i] = objective(ws[:,i], X, y, ss)\n",
    "    \n",
    "plt.figure()\n",
    "CS = plt.contour(w1, w2, obj.reshape(w1.shape), levels=np.exp(np.linspace(np.log(min(obj.flatten())), np.log(max(obj.flatten())), 12)))\n",
    "plt.plot(w[1], w[2], 'rx')\n",
    "plt.clabel(CS, inline=1, fontsize=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how it's nice and smooth. You can imagine how continuing to walk downhill from any point will lead you to the optimum, marked by the cross. This is because the surface is *convex*. But what happens if we turn off or reduce the regularisation term in the objective?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.mlab as mlab\n",
    "\n",
    "delta = 0.025\n",
    "w1, w2 = np.meshgrid(np.arange(w[1]-2.0, w[1]+2.0, delta), np.arange(w[2]-2.0, w[2]+2.0, delta))\n",
    "ws = np.vstack([w[0] * np.ones_like(w1.flatten()), w1.flatten(), w2.flatten()])\n",
    "obj = np.zeros(ws.shape[1])\n",
    "for i in range(ws.shape[1]):\n",
    "    obj[i] = objective(ws[:,i], X, y, 1e-3)\n",
    "\n",
    "plt.figure()\n",
    "CS = plt.contour(w1, w2, obj.reshape(w1.shape), levels=np.exp(np.linspace(np.log(min(obj.flatten())), np.log(max(obj.flatten())), 12)))\n",
    "#plt.plot(w[1], w[2], 'rx')\n",
    "plt.clabel(CS, inline=1, fontsize=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "See how the magnitude of the values increases dramatically. What's going on here? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
