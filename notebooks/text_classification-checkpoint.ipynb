{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's get the corpus we will be using, which is included in NLTK. You will need NLTK and Scikit-learn (as well as their dependencies, in particular scipy and numpy) to run this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"reuters\") # if necessary\n",
    "from nltk.corpus import reuters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK sample of the Reuters Corpus contains 10,788 news documents totaling 1.3 million words. The documents have been classified into 90 topics, and is divided into a training and test sets, a split which we will preserve here. Let's look at the counts of texts the various categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for category in reuters.categories():\n",
    "    print category, len(reuters.fileids(category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the documents in the corpus are tagged with multiple labels; in this situation, a straightforward approach is to build a classifier for each label. Let's build a classifier to distinguish the most common topic in the corpus, \"acq\" (acqusitions). First, here's some code to build the dataset in preparation for classification using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "def get_BOW(text):\n",
    "    BOW = {}\n",
    "    for word in text:\n",
    "        BOW[word] = BOW.get(word,0) + 1\n",
    "    return BOW\n",
    "\n",
    "def prepare_reuters_data(topic,feature_extractor):\n",
    "    training_set = []\n",
    "    training_classifications = []\n",
    "    test_set = []\n",
    "    test_classifications = []\n",
    "    for file_id in reuters.fileids():\n",
    "        feature_dict = feature_extractor(reuters.words(file_id))   \n",
    "        if file_id.startswith(\"train\"):\n",
    "            training_set.append(feature_dict)\n",
    "            if topic in reuters.categories(file_id):\n",
    "                training_classifications.append(topic)\n",
    "            else:\n",
    "                training_classifications.append(\"not \" + topic)\n",
    "        else:\n",
    "            test_set.append(feature_dict)\n",
    "            if topic in reuters.categories(file_id):\n",
    "                test_classifications.append(topic)\n",
    "            else:\n",
    "                test_classifications.append(\"not \" + topic)        \n",
    "    vectorizer = DictVectorizer()\n",
    "    training_data = vectorizer.fit_transform(training_set)\n",
    "    test_data = vectorizer.transform(test_set)\n",
    "    return training_data,training_classifications,test_data,test_classifications\n",
    "\n",
    "trn_data,trn_classes,test_data,test_classes = prepare_reuters_data(\"acq\",get_BOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code builds a sparse bag of words feature representation (a Python dictionary) for each text in the corpus (which is pre-tokenized), and places it the appropriate list depending on whether it is testing or training; a corresponding list of correct classifications is created at the same time. The scikit-learn DictVectorizer class converts Python dictionaries into the scipy sparse matrices which Scikit-learn uses; for the training set, use the fit_transform method (which fixes the total number of features in the model), and for the test set, use transform method (which ignores any features in the test set that weren't in the training set). Next, let's prepare some classifiers to test..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clfs = [KNeighborsClassifier(),DecisionTreeClassifier(),RandomForestClassifier(),\n",
    "        MultinomialNB(),LinearSVC(),LogisticRegression()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we are using default settings for all these classifiers. Let's start by doing 10-fold crossvalidation on the training set, and looking at the accuracy, recall, precision, and f1-score for each (be patient, this may take a while to complete)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def do_multiple_10foldcrossvalidation(clfs,data,classifications):\n",
    "    for clf in clfs:\n",
    "        predictions = cross_validation.cross_val_predict(clf, data,classifications, cv=10)\n",
    "        print clf\n",
    "        print \"accuracy\"\n",
    "        print accuracy_score(classifications,predictions)\n",
    "        print classification_report(classifications,predictions)\n",
    "        \n",
    "do_multiple_10foldcrossvalidation(clfs,trn_data,trn_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the classifiers are not obviously biased towards a particular task, so accuracy and f-score are nearly the same. The numbers are generally quite high, indicating that it is a fairly easy classification task. In terms of the best classifier, the clear standouts here are the SVM and Logistic Regression classifiers, while <i>k</i>NN is clearly the worst. One reason <i>k</i>NN might be doing poorly is that it is particularly susceptible to a noisy feature space with dimensions that are irrelevant to the task. Let's try to improve performance by removing stopwords and doing lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "def get_BOW_lowered_no_stopwords(text):\n",
    "    BOW = {}\n",
    "    for word in text:\n",
    "        word = word.lower()\n",
    "        if word not in stopwords:\n",
    "            BOW[word] = BOW.get(word,0) + 1\n",
    "    return BOW\n",
    "\n",
    "trn_data,trn_classes,test_data,test_classes = prepare_reuters_data(\"acq\",get_BOW_lowered_no_stopwords)\n",
    "\n",
    "do_multiple_10foldcrossvalidation(clfs,trn_data,trn_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That did improve the performance of <i>k</i>NN by about 1% accuracy, but it is still the worst classifier. Gains for other classifiers were more modest, since the scores were already high, and those classifiers are more robust to feature noise.\n",
    "\n",
    "The random forest classifier is doing worse than its reputation would suggest. The default number of decision trees (n_estimators) used in the model is only 10, which is fairly low: lets see if we can find a better number..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_to_test = [10,50,100,150]\n",
    "rfs = [RandomForestClassifier(n_estimators=n) for n in n_to_test]\n",
    "do_multiple_10foldcrossvalidation(rfs,trn_data,trn_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, more subclassifiers improved things, though the Random Forest classifier is still slightly inferior to the SVM and Logistic Regression classifiers in this BOW (i.e. large feature set) situation. \n",
    "\n",
    "Both SVM and Logistic Regression classifiers have a C parameter which controls the degree of regularization (lower C means more emphasis on regularization when optimising the model). Let's see if we can improve the performance of the Logistic Regression classifier by changing the C parameter from the default (1.0). For this parameter, a logrithmic scale is appropriate..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c_to_test = [0.001,0.01,0.1,1,10,100, 1000]\n",
    "lrcs = [LogisticRegression(C=c) for c in c_to_test]\n",
    "do_multiple_10foldcrossvalidation(lrcs,trn_data,trn_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this case, changing the parameter from the default is not desirable. When training with fairly large datasets to solve a straightforward task with a simple classifier, the effect of regularization is often minimal.\n",
    "\n",
    "Under normal circumstances we might do more parameter tuning or feature selection (and we encourage you to play around), but let's just skip to testing the classifiers on the test set and displaying the results using matplotlib...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def test_and_graph(clfs,training_data,training_classifications,test_data,test_classifications):\n",
    "    accuracies = []\n",
    "    for clf in clfs:\n",
    "        clf.fit(training_data,training_classifications)\n",
    "        predictions = clf.predict(test_data)\n",
    "        accuracies.append(accuracy_score(test_classifications,predictions))\n",
    "    print accuracies\n",
    "    p = plt.bar([num + 0.25 for num in range(len(clfs))], accuracies,0.5)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy classifying acq topic in Reuters, by classifier')\n",
    "    plt.ylim(0.5)\n",
    "    plt.xticks([num + 0.5 for num in range(len(clfs))], ('kNN', 'DT', 'RF', 'NB', 'SVM', 'LR'))\n",
    "    plt.show()\n",
    "\n",
    "test_and_graph(clfs,trn_data,trn_classes,test_data,test_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are pretty close to what we saw using cross-validation, with Logistic Regression winning out over SVMs by a tiny margin, with an impressive accuracy of 98.3%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
