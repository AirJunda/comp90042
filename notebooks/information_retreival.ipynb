{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retreival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to put together a simple retreival engine in python. Here we'll focus on boolean retreival, which works over bags of words. We won't bother about any optimisations, and just use python dicts, sets etc.\n",
    "\n",
    "Our dataset is from NLTK, using the Reuters document collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "corpus = nltk.corpus.reuters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the data, which are text documents like the one below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RIFT\n",
      "  Mounting trade friction between the\n",
      "  U.S. And Japan has raised fears among many of Asia's exporting\n",
      "  nations that the row could inflict far-reaching economic\n",
      "  damage, businessmen and officials said.\n",
      "      They told Reuter correspondents in Asian capitals a U.S.\n",
      "  Move against Japan might boost protectionist sentiment in the\n",
      "  U.S. And lead to curbs on American imports of their products.\n",
      "      But some exporters said that while the conflict would hurt\n",
      "  them in the long-run, in the short-term Tokyo's loss might be\n",
      "  their gain.\n",
      "      The U.S. Has said it will impose 300 mln dlrs of tariffs on\n",
      "  imports of Japanese electronics goods on April 17, in\n",
      "  retaliation for Japan's alleged failure to stick to a pact not\n",
      "  to sell semiconductors on world markets at below cost.\n",
      "      Unofficial Japanese estimates put the impact of the tariffs\n",
      "  at 10 billion dlrs and spokesmen for major electronics firms\n",
      "  said they would virtually halt exports\n"
     ]
    }
   ],
   "source": [
    "print corpus.raw(corpus.fileids()[0])[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to tokenise the documents, remove stop-words and stem the words to form our bag-of-words representation. Here we'll use the PorterStemmer (but be aware there are others in NLTK). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english')) # wrap in a set() (see below)\n",
    "stemmer = nltk.stem.PorterStemmer() \n",
    "\n",
    "def extract_terms(doc):\n",
    "    terms = set()\n",
    "    for token in nltk.word_tokenize(doc):\n",
    "        if token not in stopwords: # 'in' and 'not in' operations are much faster over sets that lists\n",
    "            terms.add(stemmer.stem(token.lower()))\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the method using the first document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'&', u\"''\", u\"'s\", u'(', u')', u',', u'.', u'10', u'15.6', u'17', u'1985', u'30', u'300', u'4.9', u'53', u'7.1', u'95', u';', u'>', u'``', u'a', u'abl', u'account', u'action', u'advantag', u'alleg', u'allow', u'also', u'american', u'among', u'analyst', u'and', u'april', u'asia', u'asian', u'ask', u'associ', u'australia', u'australian', u'avow', u'await', u'awar', u'barrier', u'beef', u'below-cost', u'beyond', u'biggest', u'billion', u'block', u'boost', u'broker', u'budget', u'busi', u'businessmen', u'but', u'button', u'call', u'canberra', u'capel', u'capit', u'centr', u'chairman', u'chief', u'co', u'coal', u'commerci', u'complet', u'concern', u'conflict', u'continu', u'correspond', u'cost', u'could', u'countri', u'curb', u'cut', u'damag', u'day', u'defus', u'democrat', u'deputi', u'despit', u'deterior', u'diplomat', u'director-gener', u'disadvantag', u'disput', u'dlr', u'domest', u'due', u'econom', u'economi', u'effort', u'electr', u'electron', u'emerg', u'end', u'eros', u'estim', u'exchang', u'expand', u'export', u'extend', u'failur', u'far-reach', u'fear', u'feder', u'financi', u'firm', u'first', u'fiscal', u'foreign', u'friction', u'friday', u'from', u'gain', u'good', u'govern', u'group', u'ha', u'half', u'halt', u'hard-hit', u'he', u'help', u'hit', u'hong', u'hurt', u'if', u'impact', u'import', u'impos', u'in', u'includ', u'industri', u'inflict', u'interest', u'intern', u'jame', u'japan', u'japanes', u'john', u'kind', u'kong', u'korea', u'kuroda', u'larg', u'largest', u'last', u'lawrenc', u'lead', u'length', u'liber', u'long-run', u'loss', u'lt', u'ltd', u'major', u'makoto', u'malaysia', u'mani', u'manoeuvr', u'manufactur', u'market', u'matsushita', u'matter', u'mc.t', u'mean', u'meanwhil', u'measur', u'meet', u'michael', u'might', u'mill', u'minist', u'miti', u'mln', u'month', u'mount', u'move', u'much', u'murtha', u'must', u\"n't\", u'nakason', u'name', u'nation', u'new', u'newspap', u'offic', u'offici', u'one', u'open', u'outcom', u'outlin', u'outweigh', u'packag', u'pact', u'parti', u'partner', u'paul', u'pct', u'place', u'possibl', u'pressur', u'prevent', u'prime', u'problem', u'produc', u'product', u'program', u'promot', u'propos', u'protectionist', u'public', u'purpos', u'put', u'quickli', u'rais', u'record', u'reform', u'relat', u'remain', u'remov', u'repres', u'reserv', u'restrain', u'retali', u'reuter', u'rift', u'row', u'rule', u'safe', u'said', u'sale', u'sell', u'semiconductor', u'senior', u'sentiment', u'seriou', u'serious', u'serv', u'share', u'sheen', u'short-term', u'signific', u'similar', u'smith', u'solv', u'sourc', u'south', u'spend', u'spokesman', u'spokesmen', u'stand-off', u'stepped-up', u'stick', u'stimul', u'stock', u'subject', u'supplementari', u'surplu', u'swell', u'taiwan', u'taiwanes', u'talk', u'tariff', u'tax', u'textil', u'that', u'the', u'they', u'thi', u'third', u'threat', u'time', u'tokyo', u'told', u'tom', u'tough', u'trade', u'two', u'u.', u'u.s.', u'u.s.-japan', u'u.s.-japanes', u'unoffici', u'up', u'us', u'view', u'virtual', u'want', u'warn', u'washington', u'we', u'week', u'whole', u'whose', u'work', u'world', u'worri', u'would', u'yasuhiro', u'year', u'yesterday']\n"
     ]
    }
   ],
   "source": [
    "doc = corpus.raw(corpus.fileids()[0])\n",
    "print list(sorted(extract_terms(doc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We probably want to remove numbers and punctuation, which aren't being caught by the stop list. We may want to be a bit more agressive with tokenising hyphenated words (although take care, as some might be important.) Have a go yourself and see if you can improve the preprocessing to correct for these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply the term extraction method to all documents in our corpus. (This may take a minute or two.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = {}\n",
    "for docid in corpus.fileids():\n",
    "    terms = extract_terms(corpus.raw(docid))\n",
    "    docs[docid] = terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And build an inverted index, which transposes the above data structure such that terms are the key and a set of document identifiers are the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "inverted_index = defaultdict(list)\n",
    "for docid, terms in docs.items():\n",
    "    for term in terms:\n",
    "        inverted_index[term].append(docid)\n",
    "        \n",
    "# need to keep doc lists in sorted order\n",
    "for term, docids in inverted_index.items():\n",
    "    docids.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a query term, say *'Taiwanese'*. We can retreive all documents containing this term (being careful to process the query in the same way as the document)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'test/14826',\n",
       " u'test/16214',\n",
       " u'test/19040',\n",
       " u'training/10299',\n",
       " u'training/11813',\n",
       " u'training/354',\n",
       " u'training/6976',\n",
       " u'training/7135',\n",
       " u'training/7531',\n",
       " u'training/8063',\n",
       " u'training/9007',\n",
       " u'training/9184']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_index[stemmer.stem('Taiwanese'.lower())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about a multiple term query? Consider *Taiwanese beef* as our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 67\n"
     ]
    }
   ],
   "source": [
    "postings1 = inverted_index[stemmer.stem('Taiwanese'.lower())]\n",
    "postings2 = inverted_index[stemmer.stem('beef'.lower())]\n",
    "print len(postings1), len(postings2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have to intersect the posting lists (sets here) to implement the conjuctive query. As the postings are in sorted order, we can do this efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def intersect_postings(postings1, postings2):\n",
    "    i = j = 0\n",
    "    results = []\n",
    "    while i < len(postings1) and j < len(postings2):\n",
    "        if postings1[i] < postings2[j]:\n",
    "            i += 1\n",
    "        elif postings1[i] > postings2[j]:\n",
    "            j += 1\n",
    "        else:\n",
    "            results.append(postings1[i])\n",
    "            i += 1\n",
    "            j += 1\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you see why the postings lists need to be sorted? Now we can test it on our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'test/14826']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersect_postings(postings1, postings2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might want to think about how to process queries that include more terms, and disjunctions (OR) or negations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
