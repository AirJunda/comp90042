{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to put together a simple retrieval engine in python. We'll focus on *ranked retrieval*, using a TFxIDF vector space model. We won't bother about any optimisations, and just use python dicts, sets etc.\n",
    "\n",
    "Our dataset is the *Cranfield collection*, an early resource which has been widely used in IR research. You may need to call `nltk.download()` to install the stopwords corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from math import log\n",
    "from collections import defaultdict, Counter\n",
    "import requests, tarfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, download the cranfield files, using the code below. Or you can do this manually by downloading http://ir.dcs.gla.ac.uk/resources/test_collections/cran/cran.tar.gz and extracting the files into the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download\n",
    "fname = 'cran.tar.gz'\n",
    "url = 'http://ir.dcs.gla.ac.uk/resources/test_collections/cran/' + fname\n",
    "r = requests.get(url)\n",
    "# write to disk\n",
    "open(fname , 'wb').write(r.content)\n",
    "# extract files from archive\n",
    "tar = tarfile.open(fname, \"r:\")\n",
    "tar.extractall()\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to extract the document details---the document ids and text content---from the cranfield files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_cranfield_documents(fname):\n",
    "    SKIP, READ = range(2)\n",
    "    state = SKIP\n",
    "    identifier = body = None\n",
    "    with open(fname) as infile:\n",
    "        for line in infile:\n",
    "            line = line.strip()\n",
    "            parts = line.split()\n",
    "            first = parts[0]\n",
    "            if first[0] == \".\":\n",
    "                if first[1] == \"I\":\n",
    "                    if state != None and identifier:\n",
    "                        yield (identifier, body)\n",
    "                    identifier = int(parts[1])\n",
    "                    state = SKIP\n",
    "                elif first[1] == \"W\":\n",
    "                    state = READ\n",
    "                    body = ''\n",
    "                else:\n",
    "                    state = SKIP\n",
    "            elif state == READ:\n",
    "                body += line + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docfname = 'cran.all.1400'\n",
    "raw_docs = list(parse_cranfield_documents(docfname))\n",
    "raw_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to tokenise the documents, remove stop-words and stem the words to form our bag-of-words representation. Here we'll use the PorterStemmer (there are others in NLTK). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english')) # wrap in a set() (see below)\n",
    "stemmer = nltk.stem.PorterStemmer() \n",
    "\n",
    "def extract_terms(doc):\n",
    "    terms = set()\n",
    "    for token in nltk.word_tokenize(doc):\n",
    "        if token not in stopwords: # 'in' and 'not in' operations are much faster over sets that lists\n",
    "            terms.add(stemmer.stem(token.lower()))\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the method using the first document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(sorted(extract_terms(raw_docs[0][1]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We probably want to remove numbers and punctuation, which aren't being caught by the stop list. We may want to be a bit more agressive with tokenising hyphenated words (although take care, as some might be important.) Have a go yourself and see if you can improve the preprocessing to correct for these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply the term extraction method to all documents in our corpus. (This may take a minute.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = {}\n",
    "for docid, text in raw_docs:\n",
    "    terms = extract_terms(text)\n",
    "    docs[docid] = terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a _vector space model_ we must define a scoring function capable of relating a query to a document, then use this to retrieve the top *k* scoring documents for a given query. The scoring function we use is the cosine similarity measure over a _TF*IDF_ vector representation of the document and the query. \n",
    "\n",
    "This requires us to process the data to compute:\n",
    "1. term frequencies within each document (a *bag* of words, rather than a *set* as for boolean retrieval)\n",
    "2. document frequencies for each term, counting how many documents they occur in\n",
    "3. normlised _tf*idf_ vectors for each document \n",
    "4. an inverted index to allow for efficient lookup by term\n",
    "\n",
    "The first step is to collect term frequencies for each document, which is only a slight change from the code above for creating the binary index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_term_freqs(doc):\n",
    "    tfs = Counter()\n",
    "    for token in doc:\n",
    "        if token not in stopwords: # 'in' and 'not in' operations are much faster over sets that lists\n",
    "            tfs[stemmer.stem(token.lower())] += 1\n",
    "    return tfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need to compute the document frequencies, for the *idf* factors in the scoring function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_doc_freqs(doc_term_freqs):\n",
    "    dfs = Counter()\n",
    "    for tfs in doc_term_freqs.values():\n",
    "        for term in tfs.keys():\n",
    "            dfs[term] += 1\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above two functions, process the corpus into term frequencies and document frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_term_freqs = {}\n",
    "for docid, terms in docs.items():\n",
    "    term_freqs = extract_term_freqs(terms)\n",
    "    doc_term_freqs[docid] = term_freqs\n",
    "M = len(doc_term_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_freqs = compute_doc_freqs(doc_term_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, let's check some of the *df* values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for term in \"flux viscous magnet\".split():\n",
    "    stem = stemmer.stem(term.lower())\n",
    "    print(\"term\", term, \"df\", doc_freqs[stem], 'idf', log(M / float(doc_freqs[stem])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to create the inverted index. For this we need to include the scoring function to compute the *tf* and *idf* values, then normalise each document vector to be unit length. For this we process each document in the corpus, compute a vector with one entry per term, normalise for length and store in the inverted index.\n",
    "\n",
    "Note that this a fairly inefficient inverted index, as described in lecture 16. Many optimisations are possible, and are indeed necessary for this to scale to more realistic sized datasets. The approach below, though, is fine for small collections like this one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vsm_inverted_index = defaultdict(list)\n",
    "for docid, term_freqs in doc_term_freqs.items():\n",
    "    N = sum(term_freqs.values())\n",
    "    length = 0\n",
    "    \n",
    "    # find tf*idf values and accumulate sum of squares \n",
    "    tfidf_values = []\n",
    "    for term, count in term_freqs.items():\n",
    "        tfidf = float(count) / N * log(M / float(doc_freqs[term]))\n",
    "        tfidf_values.append((term, tfidf))\n",
    "        length += tfidf ** 2\n",
    "\n",
    "    # normalise documents by length and insert into index\n",
    "    length = length ** 0.5\n",
    "    for term, tfidf in tfidf_values:\n",
    "        # note the inversion of the indexing, to be term -> (doc_id, score)\n",
    "        vsm_inverted_index[term].append([docid, tfidf / length])\n",
    "        \n",
    "# ensure posting lists are in sorted order (less important here cf above)\n",
    "for term, docids in vsm_inverted_index.items():\n",
    "    docids.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For querying this index, we use the algorithm from the lecture 16. This just sums up the weights for each document using the posting lists for all query terms, then returns the ranked list of results. (Again, there are more efficient algorithms for doing ranked retrieval in the VSM, such as [WAND](http://cis.poly.edu/westlab/papers/cntdstrb/p426-broder.pdf), covered in lecture 17.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_vsm(query, index, k=10):\n",
    "    accumulator = Counter()\n",
    "    for term in query:\n",
    "        postings = index[term]\n",
    "        for docid, weight in postings:\n",
    "            accumulator[docid] += weight\n",
    "    return accumulator.most_common(k)\n",
    "\n",
    "results = query_vsm([stemmer.stem(term.lower()) for term in \"flux viscous magnet\".split()], vsm_inverted_index)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth taking a look at the top scoring document(s), e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_docs[results[0][0]-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this document does not include all the query terms. While it has *viscous* and *magnet* it does not include *flux*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This collection contains other files, besides the lists of documents. Namely a list of sample queries, and judgements of the relevance of each document against these queries. We'll return to this in later weeks when covering IR evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
